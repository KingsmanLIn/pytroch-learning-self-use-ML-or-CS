#简单神经网络的实现
import torch.nn as nn
import torch.optim as optim
import torch
#是运行放入GPU中加速计算
device=torch.device('cuda' if torch.cuda.is_available() else 'cpu')
#定义一个简单的神经网络
class simpleNN(nn.Module):
    def __init__(self):
        super(simpleNN,self).__init__()
        self.fc1=nn.Linear(5,3)#输入层到隐藏层，输入层10神经元，隐藏层5神经元
        self.fc2=nn.Linear(3,1)#隐藏层到输出层，隐藏层5神经元，输出层1神经元

    def forward(self,x):
        x=torch.relu(self.fc1(x))#定义一个激活函数，对fc1的输出使用激活函数进行计算
        x=self.fc2(x)
        return x
#创建网络实例
model=simpleNN()
model=model.to(device)
#打印模型结构
'''print(model)'''
#现在我们输入一个样本进行测试
'''x=torch.randn(1,2)
print(x)'''
#前向传播算法对x进行处理
'''output=model(x)
print(output)'''
#之后我需要让模型进行训练，让他能够学习到输入和输出之间的关系，所以我现在需要开始实现反向传播算法
#实现一个反向传播算法，我需要损失函数和优化器进行计算
#我现在需要定义一个损失函数，我们这次使用的是MSE（均方误）
criterion=nn.MSELoss()
#现在假设一个我想要让他靠近的真实输出值
'''target=torch.ones(1,1)#创建一个为1的1*1的目标值张量
print(target)'''
#计算损失值
'''loss=criterion(output,target)
print(loss)'''
#现在我们要定义一个优化器，我们先从Adam优化器尝试
optimizer=optim.Adam(model.parameters(),lr=0.01)#lr是学习率，parameters()是模型里头的参数，该代码指的是把所有的参数导入优化器
#优化器训练过程
'''optimizer.zero_grad()#清空所有里头的梯度
loss.backward()#将损失函数的值进行反向传播进行计算
optimizer.step()#更新神经网络里头的每一个参数'''
#现在我们尝试输入样本开始训练
X=torch.randn(10,5,device=device)#样本里头有10个样本，5个特征
Y=torch.randn(10,1,device=device)#输出结果里头有10个目标值，相当于只有一列Y
#开始训练
for epoch in range(10000):#对样本进行10000次训练
    optimizer.zero_grad()
    output=model(X)#通过前向传播计算获得最终输出值
    loss=criterion(output,Y)#计算损失值
    loss.backward()#通过反向传播开始计算梯度并开始修正
    optimizer.step()#更新模型里头的参数值
    #观察结果，每训练十次输出一次loss值的差距
    if (epoch+1)%10==0:
        print(f'Epoch[{epoch+1}/10000],loss:{loss.item():.4f}')
